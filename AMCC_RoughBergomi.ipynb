{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe67fc7",
   "metadata": {
    "id": "4fe67fc7"
   },
   "source": [
    "# Monte Carlo Calibration via Deep Learning\n",
    "\n",
    "## Sample code for calibrating a rough Bergomi model to SPX options data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a03741d",
   "metadata": {
    "id": "7a03741d"
   },
   "source": [
    "This code contains a sample implementation for the *Automatic Monte Carlo Calibration (AMCC)* method introduced in the paper *Monte Carlo Calibration via Deep Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6a468",
   "metadata": {
    "id": "94d6a468"
   },
   "source": [
    "### Getting started and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-category",
   "metadata": {
    "id": "later-category"
   },
   "outputs": [],
   "source": [
    "!pip install py_vollib_vectorized\n",
    "!pip install mpmath\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import py_vollib_vectorized\n",
    "import warnings\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(1)\n",
    "from mpmath import hyp2f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-february",
   "metadata": {
    "id": "recent-february"
   },
   "source": [
    "First we need to load the relevant data. In the paper we use Data from OptionMetrics via Wharton Research Data Services (WRDS).\n",
    "\n",
    "The data file needs to contain a list of contracts with the following attributes:\n",
    "\n",
    "- **date**  (e.g. '2013-08-17')\n",
    "- **exdate** (maturity of the contract in the same format)\n",
    "- **cp_flag** ('p' for put, 'c' for call)\n",
    "- **strike**\n",
    "- **best_bid** (best bid price)\n",
    "- **best_offer** (best ask price)\n",
    "- **volume**\n",
    "- **forward_price** (SPX forward)\n",
    "- **midP** (mid price of contract)\n",
    "- **maturity** (time-to-maturity as float)\n",
    "- **Spot** \t(SPX spot)\n",
    "- **bid_vols** (implied volatility for best bid price)\n",
    "- **offer_vols** (implied volatility for best ask price)\n",
    "- **mid_vols** (implied volatility for mid price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2667f1",
   "metadata": {
    "id": "2e2667f1"
   },
   "outputs": [],
   "source": [
    "## When running on colab, upload of data works best by activating this celL:\n",
    "if False:\n",
    "    from google.colab import files\n",
    "\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    for fn in uploaded.keys():\n",
    "      print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "          name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4d7ec",
   "metadata": {
    "id": "1de4d7ec"
   },
   "outputs": [],
   "source": [
    "## Once uploaded, we load the data into a pandas dataframe.\n",
    "\n",
    "data_full = pd.read_csv('IV_data2013.csv')\n",
    "data_full = data_full.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5f9ed",
   "metadata": {
    "id": "61e5f9ed"
   },
   "outputs": [],
   "source": [
    "dates = data_full.drop_duplicates('date')['date'].tolist()\n",
    "print(dates)\n",
    "## Select the relevant dates here\n",
    "dates = dates[0:25]\n",
    "data_full.allDates = dates\n",
    "data_full.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc760545",
   "metadata": {
    "id": "cc760545"
   },
   "outputs": [],
   "source": [
    "## Only use part of the data for training: specify volume threshold here.\n",
    "threshold =  1000\n",
    "data = data_full[data_full['volume']>threshold]\n",
    "data=data.dropna()\n",
    "data.allDates = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04fb5a",
   "metadata": {
    "id": "aa04fb5a"
   },
   "outputs": [],
   "source": [
    "data.count()[0]/data_full.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf8df3",
   "metadata": {
    "id": "06cf8df3"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c833a9",
   "metadata": {
    "id": "87c833a9"
   },
   "source": [
    "### Defining auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1b1b1",
   "metadata": {
    "id": "50f1b1b1"
   },
   "outputs": [],
   "source": [
    "def covariance_matrix(H,M,T):\n",
    "    \"\"\" Computes the covariance matrix for the exact rough Bergomi simluation scheme.\n",
    "\n",
    "    Args:\n",
    "        H: Hurst parameter (float)\n",
    "        M: number of time steps (int)\n",
    "        T: maturity (float)\n",
    "    Returns:\n",
    "        np.ndarray: covariance matrix Shape (2M,2M)\n",
    "    \"\"\"\n",
    "\n",
    "    h = T/M\n",
    "    mat = np.zeros((2*M,2*M))\n",
    "    gamma_aux = 0.5 - H\n",
    "    frac_aux = 1.0/(1.0-gamma_aux)\n",
    "\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "\n",
    "            mm = min(i,j) +1\n",
    "            mm_x = max(i,j) +1\n",
    "\n",
    "            mat[2*i,2*j+1] = (  ((i+1)*h)**(H+0.5) - ( (i+1)*h  - mm*h )**(H+0.5) )/(H+0.5)\n",
    "\n",
    "            mat[2*i,2*j] = ((mm*h)**(2*H))*hyp2f1(1.0, gamma_aux, 2-gamma_aux,mm/mm_x)*frac_aux*(mm/mm_x)**gamma_aux\n",
    "\n",
    "            mat[2*i+1,2*j+1] = (mm*h)\n",
    "\n",
    "            mat[2*i+1,2*j] = (  ((j+1)*h)**(H+0.5) - ( (j+1)*h  - mm*h )**(H+0.5) )/(H+0.5)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "def kernel(time_points,hurst_exp):\n",
    "\n",
    "    kernels = time_points**(hurst_exp-0.5)\n",
    "\n",
    "    return kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5058af9",
   "metadata": {
    "id": "a5058af9"
   },
   "source": [
    "Next we build two dictionaries that will be useful when accessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5aa7d",
   "metadata": {
    "id": "b5b5aa7d"
   },
   "outputs": [],
   "source": [
    "df = data_full[data_full['date']==dates[0]]\n",
    "exdates = df.drop_duplicates('exdate')['exdate'].tolist()\n",
    "print(exdates)\n",
    "\n",
    "dfDict = {}\n",
    "for t in data_full.allDates:\n",
    "    dfT = data_full.loc[data_full['date'] == t]\n",
    "    allMats = dfT.drop_duplicates('exdate')['exdate'].tolist()\n",
    "    dfDict[t] = []\n",
    "    for tau in allMats:\n",
    "        dfTtau = dfT[dfT[\"exdate\"] == tau]\n",
    "        dfDict[t].append((dfTtau, tau))\n",
    "data_full.dfDict=dfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202cfef",
   "metadata": {
    "id": "a202cfef"
   },
   "outputs": [],
   "source": [
    "df = data[data['date']==dates[0]]\n",
    "exdates = df.drop_duplicates('exdate')['exdate'].tolist()\n",
    "print(exdates)\n",
    "\n",
    "for exdate in exdates:\n",
    "    df_tau = df[df['exdate']==exdate]\n",
    "\n",
    "dfDict = {}\n",
    "for t in data.allDates:\n",
    "    dfT = data.loc[data['date'] == t]\n",
    "    allMats = dfT.drop_duplicates('exdate')['exdate'].tolist()\n",
    "    dfDict[t] = []\n",
    "    for tau in allMats:\n",
    "        dfTtau = dfT[dfT[\"exdate\"] == tau]\n",
    "        dfDict[t].append((dfTtau, tau))\n",
    "data.dfDict=dfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2781a",
   "metadata": {
    "id": "5df2781a"
   },
   "source": [
    "Check how many datapoints you are using approximately per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d372d6",
   "metadata": {
    "id": "c3d372d6"
   },
   "outputs": [],
   "source": [
    "counter = 0.\n",
    "for i in range(len(dates)):\n",
    "    counter = counter + data[data['date'] == dates[i]].count()[0]\n",
    "print(counter / len(dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e296aa",
   "metadata": {
    "id": "70e296aa"
   },
   "source": [
    "Define the neural network that could be used as forward variance curve parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49423838",
   "metadata": {
    "id": "49423838"
   },
   "outputs": [],
   "source": [
    "hidden_size=10\n",
    "class FNN_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, 1),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.linear_relu_stack(x)\n",
    "        return out\n",
    "FNN = FNN_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488c247",
   "metadata": {
    "id": "4488c247"
   },
   "source": [
    "### Calibration\n",
    "\n",
    "Here we define the object used for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c080f63",
   "metadata": {
    "id": "2c080f63"
   },
   "outputs": [],
   "source": [
    "class Rough_Bergomi_calibration():\n",
    "\n",
    "    def __init__(self,select_date_ind=0):\n",
    "        self.v0 =  torch.tensor(0.02)\n",
    "        self.eta =  torch.tensor(2.3)\n",
    "        self.iter_numb = 1\n",
    "        self.learning_rate = 0.01\n",
    "        self.errorMessage = False\n",
    "        self.batch_size = torch.tensor(25000)\n",
    "        self.time_steps = 50\n",
    "        self.time_steps_t = torch.tensor(self.time_steps)\n",
    "        self.r0=torch.tensor(0.0)\n",
    "        self.xi=torch.ones(30)\n",
    "        self.hurst = torch.tensor(0.05)\n",
    "        self.rho = torch.tensor(-0.9)\n",
    "        self.sqrt_rho = torch.sqrt(1-self.rho**2)\n",
    "        self.use_NN_fwc = True\n",
    "        self.use_exact_sim = True\n",
    "        self.eval_exact = True\n",
    "        self.select_date_ind = select_date_ind\n",
    "        if self.use_exact_sim:\n",
    "            self.build_cholesky(self.select_date_ind)\n",
    "\n",
    "\n",
    "    def select_data(self, data, select_date_ind, select_mat_ind):\n",
    "        '''\n",
    "        Load data for date specified by index and maturity specified by index and set\n",
    "        object parameters (time-grid, maturity,...) accordingly.\n",
    "        '''\n",
    "        if (select_date_ind >= 0) & (select_date_ind < len(data.allDates)):\n",
    "\n",
    "            self.o = data.allDates[select_date_ind] ## Select one particular day\n",
    "\n",
    "            if (select_mat_ind >= 0) & (select_mat_ind < len(data.dfDict[self.o])):\n",
    "\n",
    "                df_extract = data.dfDict[self.o][select_mat_ind]\n",
    "            else:\n",
    "                self.errorMessage = True\n",
    "        else:\n",
    "            self.errorMessage = True\n",
    "\n",
    "        if self.errorMessage:\n",
    "            print(\"No data available. Please choose other\")\n",
    "\n",
    "        else:\n",
    "            self.Tau = df_extract[1]\n",
    "            df = df_extract[0]\n",
    "\n",
    "            self.data = df\n",
    "            self.forward = torch.tensor(np.array(self.data[\"forward_price\"])[0])\n",
    "            self.strikes = torch.tensor(np.array(self.data[\"strike\"]))\n",
    "            self.ivs_market = torch.tensor(np.array(self.data[\"mid_vols\"]))\n",
    "            self.ivs_bid = torch.tensor(np.array(self.data[\"bid_vols\"]))\n",
    "            self.ivs_offer = torch.tensor(np.array(self.data[\"offer_vols\"]))\n",
    "            self.mids = self.data[\"midP\"]\n",
    "            self.bids = self.data['best_bid']\n",
    "            self.offers = self.data['best_offer']\n",
    "            self.spot = torch.tensor(np.array(self.data['Spot'])[0])\n",
    "            self.maturity = torch.tensor(np.array(self.data[\"maturity\"].iloc[0]))\n",
    "            self.h = torch.tensor(self.maturity/self.time_steps).float()\n",
    "            self.pc_factor = torch.tensor(np.array(2.*(df_extract[0][\"cp_flag\"] == \"c\")-1.))\n",
    "            self.logStrikes = -np.log(df[\"forward_price\"] / df[\"strike\"])\n",
    "            if self.use_NN_fwc:\n",
    "                self.variance_curve = self.forward_variance_curve_nn()\n",
    "            else:\n",
    "                self.variance_curve = self.forward_variance_curve()\n",
    "            self.cp_flag = df_extract[0][\"cp_flag\"]\n",
    "            self.cholesky_mat = self.chol_list[select_mat_ind]\n",
    "\n",
    "    def build_cholesky(self,select_date_ind):\n",
    "        '''\n",
    "        Compute Cholesky decomposition of covariance matrix for all maturities for a given date.\n",
    "        '''\n",
    "        self.chol_list = []\n",
    "        for i in range(len(dfDict[data.allDates[select_date_ind]])):\n",
    "            o = data.allDates[select_date_ind]\n",
    "            df_extract = data.dfDict[o][i]\n",
    "            maturity = torch.tensor(np.array(df_extract[0][\"maturity\"].iloc[0]))\n",
    "            cholesky_mat_tmp = covariance_matrix(self.hurst.detach().numpy(),self.time_steps,maturity.detach().numpy())\n",
    "            cholesky_mat = torch.tensor(np.linalg.cholesky(cholesky_mat_tmp)).to(torch.float32)\n",
    "            self.chol_list.append(cholesky_mat)\n",
    "\n",
    "\n",
    "\n",
    "    def forward_variance_curve(self):\n",
    "        '''\n",
    "        Evaluate piecewise linear forward variance curve at time points defined by current grid.\n",
    "        '''\n",
    "        variance_curve = torch.ones(self.time_steps)\n",
    "        time_grid = self.h*torch.arange(1,self.time_steps+1)\n",
    "        for k in range(10):\n",
    "            indices = time_grid>k*0.0025\n",
    "            indices[time_grid<=(k-1)*0.0025]=False\n",
    "            variance_curve[indices] = torch.abs(self.xi[k])\n",
    "        for k in range(1,10):\n",
    "            indices = time_grid>k*0.025\n",
    "            indices[time_grid<=(k-1)*0.025]=False\n",
    "            variance_curve[indices] = torch.abs(self.xi[k+9])\n",
    "        for k in range(1,10):\n",
    "            indices = time_grid>k*0.25\n",
    "            indices[time_grid<=(k-1)*0.25]=False\n",
    "            variance_curve[indices] = torch.abs(self.xi[k+18])\n",
    "        return variance_curve\n",
    "\n",
    "    def forward_variance_curve_nn(self):\n",
    "        '''\n",
    "        Evaluate neural network forward variance curve at time points defined by current grid.\n",
    "        '''\n",
    "\n",
    "        variance_curve = torch.ones(self.time_steps)\n",
    "        time_grid = (self.h*torch.arange(1,self.time_steps+1)).reshape(self.time_steps,1)\n",
    "        variance_curve = FNN(time_grid)\n",
    "        return variance_curve\n",
    "\n",
    "    def payoff(self,samples):\n",
    "        '''\n",
    "        Evaluate put / call payoffs\n",
    "        '''\n",
    "\n",
    "        diff_tmp = samples.reshape(-1,1)-self.strikes\n",
    "        diff_tmp = diff_tmp * self.pc_factor\n",
    "        diff_tmp [diff_tmp  < 0] = 0\n",
    "\n",
    "        return diff_tmp\n",
    "\n",
    "    def simulated_paths_rough_bergomi(self,states_in,path_bm,path_bm_2):\n",
    "        '''\n",
    "        Generate sample paths of the rough Bergomi model using the approximate sampling scheme\n",
    "        '''\n",
    "\n",
    "        vec_dim = path_bm_2.shape\n",
    "        time_steps = vec_dim[1]\n",
    "\n",
    "        log_states = torch.log(states_in)\n",
    "        vol_states = torch.ones(vec_dim[0])*self.v0\n",
    "\n",
    "\n",
    "        for i in range(self.time_steps):\n",
    "\n",
    "            vec_tmp = self.h*(i + 1 - torch.arange(0, i+1))\n",
    "            kernels = kernel(vec_tmp,self.hurst)\n",
    "            fbm = torch.matmul(path_bm[:,0:(i+1)],kernels)\n",
    "\n",
    "            brownian_incr_v = path_bm[:,i]\n",
    "            brownian_incr_s = self.rho*brownian_incr_v + self.sqrt_rho*path_bm_2[:,i]\n",
    "\n",
    "            log_states  = log_states + (self.r0-0.5*vol_states)*self.h+torch.sqrt(vol_states)*brownian_incr_s\n",
    "            vol_states =  self.v0*self.variance_curve[i]*torch.exp(self.eta*torch.sqrt(2*self.hurst)*fbm - ((i+1)*self.h)**(2*self.hurst)*(self.eta**2)/2)\n",
    "\n",
    "\n",
    "        return torch.exp(log_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def call_rough_bergomi_anti(self,num_samples):\n",
    "        '''\n",
    "        Compute option prices in the rough Bergomi model using the approximate sampling scheme\n",
    "        '''\n",
    "\n",
    "        init_states  = torch.ones(num_samples)*self.spot\n",
    "        paths_bm_test = torch.normal(0, 1, (num_samples,self.time_steps_t))*torch.sqrt(self.h)\n",
    "        paths_bm_test_2 = torch.normal(0, 1, (num_samples,self.time_steps_t))*torch.sqrt(self.h)\n",
    "\n",
    "        samples_test= self.simulated_paths_rough_bergomi(init_states,paths_bm_test,paths_bm_test_2)\n",
    "        samples_test_anti= self.simulated_paths_rough_bergomi(init_states,-paths_bm_test,-paths_bm_test_2)\n",
    "\n",
    "\n",
    "\n",
    "        return (torch.mean(torch.transpose(self.payoff(samples_test),0,1),dim=1) + torch.mean(torch.transpose(self.payoff(samples_test_anti),0,1),dim=1))/2\n",
    "\n",
    "    def simulated_paths_rough_bergomi_exact(self,states_in,path_bm,vec_normal):\n",
    "        '''\n",
    "        Generate sample paths of the rough Bergomi model using the exact sampling scheme\n",
    "        '''\n",
    "\n",
    "        vec_dim = path_bm.shape\n",
    "        time_steps = vec_dim[1]\n",
    "\n",
    "        log_states = torch.log(states_in)\n",
    "        vol_states = torch.ones(vec_dim[0])*torch.abs(self.v0)\n",
    "        #vec_normal is of size (2M,N)\n",
    "        #cov_cholesky is lower diagonal of size (2M,2M)\n",
    "\n",
    "        #columns contain, for each n \\in [[1,N]], the correlated Gaussian RV\n",
    "        prod_mat = torch.transpose(torch.mm(self.cholesky_mat, vec_normal),0,1)\n",
    "        tmp = torch.zeros(1)\n",
    "\n",
    "        for i in range(self.time_steps):\n",
    "            fbm = prod_mat[:,2*i]\n",
    "            brownian_incr_v = prod_mat[:,2*i+1] - prod_mat[:,(2*(i-1)+1)]*tmp\n",
    "            brownian_incr_s = self.rho*brownian_incr_v + self.sqrt_rho*path_bm[:,i]\n",
    "            log_states  = log_states + (self.r0-0.5*vol_states)*self.h+torch.sqrt(vol_states)*brownian_incr_s\n",
    "            vol_states =  torch.abs(self.v0)*torch.abs(self.variance_curve[i])*torch.exp(self.eta*torch.sqrt(2*self.hurst)*fbm - ((i+1)*self.h)**(2*self.hurst)*(self.eta**2)/2)\n",
    "            vol_states = torch.maximum(vol_states,torch.tensor(0.))\n",
    "            tmp = 1\n",
    "\n",
    "\n",
    "        return torch.exp(log_states)\n",
    "\n",
    "\n",
    "\n",
    "    def call_rough_bergomi_exact(self,num_samples):\n",
    "        '''\n",
    "        Compute option prices in the rough Bergomi model using the exact sampling scheme\n",
    "\n",
    "        '''\n",
    "        init_states  = torch.ones(num_samples)*self.spot\n",
    "        paths_bm_test = torch.normal(0, 1, (num_samples,self.time_steps_t))*torch.sqrt(self.h)\n",
    "        vec_normal =  torch.normal(0, 1, (2*self.time_steps_t,num_samples))\n",
    "        samples_test= self.simulated_paths_rough_bergomi_exact(init_states,paths_bm_test,vec_normal)\n",
    "\n",
    "\n",
    "        return torch.mean(torch.transpose(self.payoff(samples_test),0,1),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    def AMCC_optim(self,iter_numb,select_date_ind=0):\n",
    "        '''\n",
    "        Perform Monte Carlo Calibration using AMCC\n",
    "        '''\n",
    "\n",
    "        self.v0.requires_grad = True\n",
    "        self.eta.requires_grad = True\n",
    "        self.rho.requires_grad= True\n",
    "        if self.use_exact_sim:\n",
    "            self.hurst.requires_grad = False\n",
    "            param_list = [self.v0,self.eta,self.rho]\n",
    "        else:\n",
    "            self.hurst.requires_grad = True\n",
    "            param_list = [self.v0,self.eta,self.rho,self.hurst]\n",
    "\n",
    "        if self.use_NN_fwc:\n",
    "            param_list = list(FNN.parameters())+param_list\n",
    "        else:\n",
    "            self.xi.requires_grad = True\n",
    "            param_list = param_list + [self.xi]\n",
    "\n",
    "        optimizer = torch.optim.Adam(param_list, lr=self.learning_rate)\n",
    "        FNN.train()\n",
    "        for itr in range(iter_numb):\n",
    "            loss = torch.tensor(0.)\n",
    "            for i in range(len(dfDict[data.allDates[select_date_ind]])):\n",
    "                self.select_data(data,select_date_ind,i)\n",
    "                if self.use_exact_sim:\n",
    "                    prices = self.call_rough_bergomi_exact(self.batch_size)\n",
    "                else:\n",
    "                    prices = self.call_rough_bergomi_anti(self.batch_size)\n",
    "                market_data = torch.tensor(np.array(self.mids))\n",
    "                difference = (torch.exp(-self.r0*self.maturity)*prices - market_data)\n",
    "                loss =loss + torch.sum((difference)**2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if itr%3==0:\n",
    "                print(\"Epoch number: {} and the loss : {}\".format(itr,loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def AMCC_calibrate(self, plot_maturities = [0,1],print_options = [True, True]):\n",
    "        '''\n",
    "        Carry out AMCC and print results\n",
    "        '''\n",
    "\n",
    "        self.AMCC_optim(self.iter_numb,self.select_date_ind)\n",
    "        if print_options[0]:\n",
    "            print(\"Optimal parameters: \")\n",
    "            print(\"v_0 = \", self.v0.detach().numpy().item(0))\n",
    "            print(\"\\eta = \", self.eta.detach().numpy().item(0))\n",
    "            print(\"\\rho = \", self.rho.detach().numpy().item(0))\n",
    "            print(\"H = \", self.hurst.detach().numpy().item(0))\n",
    "        if print_options[1]:\n",
    "            for maturities_ind in plot_maturities:\n",
    "                date = dates[self.select_date_ind]\n",
    "                self.select_data(data,self.select_date_ind,maturities_ind)\n",
    "                market_data = torch.tensor(np.array(self.ivs_market))\n",
    "                if RB.eval_exact:\n",
    "                    cholesky_mat_tmp = covariance_matrix(self.hurst.detach().numpy(),self.time_steps,self.maturity.detach().numpy())\n",
    "                    self.cholesky_mat = torch.tensor(np.linalg.cholesky(cholesky_mat_tmp)).to(torch.float32)\n",
    "\n",
    "                model_prices = self.evaluate(300000)\n",
    "\n",
    "                iv_RB = py_vollib_vectorized.implied_volatility.vectorized_implied_volatility(model_prices.detach().numpy(),np.array(self.spot),np.array(self.strikes),np.array(self.maturity),np.array(self.r0),self.cp_flag)\n",
    "                plt.figure(figsize=(6, 3))\n",
    "                plt.plot(self.logStrikes, self.ivs_market, 'bo', fillstyle='none',label='Market')\n",
    "                plt.plot(self.logStrikes, iv_RB, 'g+',label='RB')\n",
    "                plottitle = \"Date: \" + str(date)\n",
    "                plottitle+= \" || maturity: \" + str(self.Tau)\n",
    "                plt.title(plottitle)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                plt.figure(figsize=(6, 3))\n",
    "                plt.plot(self.logStrikes, self.mids, 'bo', fillstyle='none',label='Market')\n",
    "                plt.plot(self.logStrikes, model_prices.detach().numpy(), 'g+',label='RB')\n",
    "                plottitle = \"Date: \" + str(date)\n",
    "                plottitle+= \" || maturity: \" + str(self.Tau)\n",
    "                plt.title(plottitle)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                print(np.mean(np.array(iv_RB)-np.array(self.ivs_market))**2)\n",
    "\n",
    "    def evaluate(self,test_num_samples,select_date_ind=0):\n",
    "        FNN.eval()\n",
    "        with torch.no_grad():\n",
    "            if self.eval_exact:\n",
    "                model_prices = torch.exp(-self.r0*self.maturity)*self.call_rough_bergomi_exact(test_num_samples)\n",
    "            else:\n",
    "                model_prices = torch.exp(-self.r0*self.maturity)*self.call_rough_bergomi_anti(test_num_samples)\n",
    "        return model_prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_RMSE(data,model,show_plots=False):\n",
    "    '''\n",
    "       Evaluate model fit to data.\n",
    "    '''\n",
    "    select_date_ind = model.select_date_ind\n",
    "    with torch.no_grad():\n",
    "        loss = 0.\n",
    "        count = 0.\n",
    "        date = data.allDates[select_date_ind]\n",
    "        hits_count = 0.\n",
    "        mape = 0.\n",
    "        delta_s = 0.\n",
    "        price_metric = 0.\n",
    "        model.chol_list = []\n",
    "        for i in range(len(dfDict[date])):\n",
    "            model.chol_list.append(0.)\n",
    "            model.select_data(data,select_date_ind,i)\n",
    "            cholesky_mat_tmp = covariance_matrix(model.hurst.detach().numpy(),model.time_steps,model.maturity.detach().numpy())\n",
    "            model.cholesky_mat = torch.tensor(np.linalg.cholesky(cholesky_mat_tmp)).to(torch.float32)\n",
    "            model_prices = (model.evaluate(100000)).detach().numpy()\n",
    "            implied_vols = py_vollib_vectorized.implied_volatility.vectorized_implied_volatility(model_prices,np.array(model.forward),np.array(model.strikes),np.array(model.maturity),np.array(model.r0),model.cp_flag)\n",
    "            market_data = np.array(model.ivs_market)\n",
    "            market_prices = np.array(model.mids)\n",
    "            loss =loss + np.sum((np.array(implied_vols) - market_data)**2)#,dim=1)\n",
    "            mape = mape + np.sum(np.abs((np.array(implied_vols) - market_data))/(market_data))#,dim=1)\n",
    "            count = count + market_data.shape[0]\n",
    "            if show_plots:\n",
    "                plt.figure(figsize=(6, 3))\n",
    "                plt.plot(model.logStrikes, market_data, 'bo', fillstyle='none',label='Mid')\n",
    "                plt.plot(model.logStrikes, np.array(model.ivs_bid), 'rx', fillstyle='none',label='Bid')\n",
    "                plt.plot(model.logStrikes, np.array(model.ivs_offer), 'mx', fillstyle='none',label='Ask')\n",
    "                plt.plot(model.logStrikes, implied_vols, 'g+',label='Rough Bergomi')\n",
    "                plottitle = \"Date: \" + str(date)\n",
    "                plottitle+= \" || maturity: \" + str(model.Tau)\n",
    "                plt.title(plottitle)\n",
    "                plt.legend()\n",
    "                plt.xlabel('Log-moneyness')\n",
    "                plt.ylabel('Implied Volatility')\n",
    "                nameFile = \"RoughBergomiSmiles_\" +str(date)+ \"_\" + str(model.Tau)+\".png\" #str(pd.to_datetime(model.Tau)-pd.to_datetime(dates[0])) + \".png\"\n",
    "                #nameFile = \"RoughBergomiSmiles_\" +str(date)+ \"_\" + str(pd.to_datetime(model.Tau)-pd.to_datetime(dates[0])) + \".png\"\n",
    "                plt.savefig(nameFile,bbox_inches='tight')\n",
    "                plt.show()\n",
    "            hits = (np.array(model.ivs_bid)<np.array(implied_vols).T)*(np.array(model.ivs_offer)>np.array(implied_vols).T)\n",
    "            print(np.sum(hits)/implied_vols.shape[0])\n",
    "            hits_count = hits_count + np.sum(hits)\n",
    "            price_metric = price_metric + np.sum(((model_prices-market_prices)/np.array(model.spot))<(1./100.))\n",
    "            spread = np.array(model.offers)-np.array(model.bids)\n",
    "            delta_s = delta_s + np.sum(2*np.abs(model_prices-np.array(model.mids))/spread)\n",
    "            #print(np.sum(hits))\n",
    "    print('Loss:',np.sqrt(loss/count)) ## Mean-squared implied volatility error\n",
    "    print('MAPE:',mape/count) ## Mean absolute percentage error\n",
    "    print('Hits:', hits_count/count) ## Hits\n",
    "    print('Delta_spread:',delta_s/count) ## Error relative to spread\n",
    "    print('Market metric:',price_metric/count)\n",
    "    return((np.sqrt(loss/count))), hits_count/count, delta_s/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78186c3c",
   "metadata": {
    "id": "78186c3c"
   },
   "outputs": [],
   "source": [
    "## Initiate calibration\n",
    "RB = Rough_Bergomi_calibration()\n",
    "RB.use_NN_fwc = True\n",
    "RB.iter_numb = 500\n",
    "RB.learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc0536",
   "metadata": {
    "id": "eccc0536"
   },
   "outputs": [],
   "source": [
    "## To calibrate the Hurst parameter first, we may want to run this cell first:\n",
    "if False:\n",
    "    RB.use_exact_sim = False\n",
    "    RB.eval_exact =False\n",
    "    RB.AMCC_calibrate(print_options = [True, False])\n",
    "    RB.build_cholesky(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143f29b-401e-47b9-942f-0dc73881a866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da902069",
   "metadata": {
    "id": "da902069",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Carry out calibration with fixed Hurst parameter\n",
    "RB.use_exact_sim = True\n",
    "RB.eval_exact =True\n",
    "maturities_list = list(range(len(dfDict[data.allDates[0]])))\n",
    "RB.AMCC_calibrate(print_options = [True, True], plot_maturities=maturities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b723201",
   "metadata": {
    "id": "3b723201"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "loss,hits,delta = evaluate_RMSE(data_full,RB,show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e5be6-f7c0-4d61-9e3a-f7aeec51c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set to true to evaluate using the initial parameters. \n",
    "if False:  \n",
    "    RB.eta =  torch.tensor(2.3)\n",
    "     \n",
    "    RB.rho = torch.tensor(-0.9)\n",
    "    RB.sqrt_rho = torch.sqrt(1-RB.rho**2)\n",
    "    \n",
    "    \n",
    "    ## Evaluate performance on original parameters. \n",
    "    RB.use_exact_sim = True\n",
    "    RB.eval_exact =True\n",
    "    loss,hits,delta = evaluate_RMSE(data_full,RB,show_plots=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad106d",
   "metadata": {
    "id": "eaad106d"
   },
   "outputs": [],
   "source": [
    "### Calibrate also for subsequent dates, using the previous parameters as starting point.\n",
    "\n",
    "loss_list = [loss]\n",
    "hits_list = [hits]\n",
    "delta_list = [delta]\n",
    "for i in range(1,len(dates)):\n",
    "    RB.select_date_ind = i\n",
    "    RB.build_cholesky(RB.select_date_ind)\n",
    "    maturities_list = list(range(len(dfDict[data.allDates[i]])))\n",
    "    RB.AMCC_calibrate(print_options = [True, False], plot_maturities=maturities_list)\n",
    "    loss,hits,delta = evaluate_RMSE(data_full,RB,show_plots=False)\n",
    "    loss_list.append(loss)\n",
    "    hits_list.append(hits)\n",
    "    delta_list.append(delta)\n",
    "print(loss_list)\n",
    "print(hits_list)\n",
    "print(delta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34259a68",
   "metadata": {
    "id": "34259a68"
   },
   "outputs": [],
   "source": [
    "### Print and plot the MSE\n",
    "plt.figure()\n",
    "plt.plot(dates,loss_list,'gx')\n",
    "tickslist=[dates[i] for i in range(0,len(dates),3)]\n",
    "tickslist.append(dates[-1])\n",
    "plt.xticks(tickslist)\n",
    "plt.ylabel('Mean-squared error')\n",
    "plt.savefig('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9c8b0",
   "metadata": {
    "id": "17d9c8b0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = np.array([loss_list,hits_list,delta_list])\n",
    "\n",
    "df = pd.DataFrame(results.T)\n",
    "\n",
    "df.columns=['Loss','Hits','Delta']\n",
    "\n",
    "df.to_csv('Results1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54f003",
   "metadata": {
    "id": "6e54f003"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200404fc-1416-4a0f-9610-44f7d1a9321f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
